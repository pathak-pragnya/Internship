{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Assignment7_rnn_gru.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fS2it3qZ2DFO"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Input, Bidirectional,Embedding\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DN9_VO1LoUiG",
        "outputId": "959f27ce-cdc5-4dde-f4d7-8af3dac21651"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt' , 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "1130496/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LWuoWmkoeQH",
        "outputId": "b7bdfb84-0e44-4058-a0e2-0f9e8779e76e"
      },
      "source": [
        "text = open(path_to_file , 'rb').read().decode( encoding='utf-8' )\n",
        "\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Rn3ocgiokAJ",
        "outputId": "9eed8e93-98e2-4cef-d38f-71d65dcf02d8"
      },
      "source": [
        "print(text[:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GajFEBpvooJN",
        "outputId": "acfae3fc-1467-4767-a51f-5c4c80131fa2"
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZ94KiLFpESZ"
      },
      "source": [
        "ids_from_chars = preprocessing.StringLookup(vocabulary = list(vocab),\n",
        "                                            mask_token=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaTV0pj1qcnM"
      },
      "source": [
        "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etRk2-2mpaUS"
      },
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmEddRsUpdxy",
        "outputId": "59225804-c9b8-403f-a859-41fb80b03e04"
      },
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text , 'UTF-8'))\n",
        "all_ids"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSCp2G-XqJiv"
      },
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxlt-grfqOfh",
        "outputId": "63a601e1-0481-42f3-fc59-759392483379"
      },
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "  print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_20mgH4qTR9"
      },
      "source": [
        "seq_length = 50\n",
        "examples_per_epoch = len(text)//(seq_length+1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7DPnMMQqn8F",
        "outputId": "cc3b01fa-5319-4b1f-f43c-b4d991f3cbbe"
      },
      "source": [
        "sequences = ids_dataset.batch(seq_length+1 , drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' '], shape=(51,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGKZc19pq0aa"
      },
      "source": [
        "def split_input_target(sequence):\n",
        "  input_text = sequence[:-1]\n",
        "  target_text = sequence[1:]\n",
        "  return input_text , target_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi0zGo6jrJg_"
      },
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_37Aoq0DrPFM",
        "outputId": "dac3b2c4-b259-476d-cc42-ee7e6ac8a957"
      },
      "source": [
        "for input_example , target_example in dataset.take(1):\n",
        "  print('Input: ',text_from_ids(input_example).numpy())\n",
        "  print('Target: ',text_from_ids(target_example).numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:  b'First Citizen:\\nBefore we proceed any further, hear'\n",
            "Target:  b'irst Citizen:\\nBefore we proceed any further, hear '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c98pCOtKrZzR"
      },
      "source": [
        "batch_size = 64\n",
        "\n",
        "buffer_size = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(buffer_size)\n",
        "    .batch(batch_size,drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Xtbl5i3sWAx"
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "\n",
        "embedding_dim = 256\n",
        "\n",
        "rnn_units = 1024"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvzI0JuMei5T"
      },
      "source": [
        "def loss_plot(history):\n",
        "  plt.xlabel(\"Number of Epochs\")\n",
        "  plt.ylabel('Loss', fontsize=16)\n",
        "  plt.plot(history.history['loss'], color='b', label='Training Loss')\n",
        "  plt.legend(loc='upper right')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTejSbyZZSI8"
      },
      "source": [
        "# RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeKf-h65ZROQ"
      },
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.SimpleRNN(rnn_units,\n",
        "                                   activation='tanh',\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True\n",
        "                                   )\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x,states\n",
        "    else:\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cD5HDmh-ZQ-C"
      },
      "source": [
        "rnn_model = MyModel(\n",
        "    vocab_size = len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JypyGCavZQ6k",
        "outputId": "a257abd6-459a-4d1a-efb2-9b7da203d9ce"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_preds = rnn_model(input_example_batch)\n",
        "  print(example_batch_preds.shape, '#(batch_size, sequence_length, vocab_size)')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(64, 50, 66) #(batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pd_02UOfZQyQ",
        "outputId": "45cc779e-f0f5-4c50-c7ed-df3f9e4528fd"
      },
      "source": [
        "rnn_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        multiple                  16896     \n",
            "_________________________________________________________________\n",
            "simple_rnn (SimpleRNN)       multiple                  1311744   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  67650     \n",
            "=================================================================\n",
            "Total params: 1,396,290\n",
            "Trainable params: 1,396,290\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVgzAFGZZQpD"
      },
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N90NibvKZQlp"
      },
      "source": [
        "opt = keras.optimizers.Adam()\n",
        "rnn_model.compile(optimizer=opt, loss=loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObT48uwpZ4Xo"
      },
      "source": [
        "checkpoint_dir = './training_checkpoints_rnn'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True,\n",
        "    save_freq = 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mvCv9AfaY76"
      },
      "source": [
        "epochs = 200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQa5SEMjaYu_",
        "outputId": "cf2fe03d-22d8-408d-ebcd-237da6407744"
      },
      "source": [
        "early_stopping = EarlyStopping(monitor = 'loss', patience = 10, mode = 'min', verbose=2)\n",
        "reduce_lr =  ReduceLROnPlateau(monitor='loss', factor=0.2,\n",
        "                              patience=5, min_lr=0.000001, verbose=2)\n",
        "history_rnn = rnn_model.fit(dataset, epochs=epochs, callbacks=[early_stopping, reduce_lr, checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "341/341 [==============================] - 28s 76ms/step - loss: 2.5294\n",
            "Epoch 2/200\n",
            "341/341 [==============================] - 26s 75ms/step - loss: 1.9476\n",
            "Epoch 3/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 1.7391\n",
            "Epoch 4/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 1.6216\n",
            "Epoch 5/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 1.5515\n",
            "Epoch 6/200\n",
            "341/341 [==============================] - 26s 75ms/step - loss: 1.5015\n",
            "Epoch 7/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 1.4663\n",
            "Epoch 8/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 1.4386\n",
            "Epoch 9/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 1.4139\n",
            "Epoch 10/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 1.3952\n",
            "Epoch 11/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 1.3771\n",
            "Epoch 12/200\n",
            "341/341 [==============================] - 26s 75ms/step - loss: 1.3624\n",
            "Epoch 13/200\n",
            "341/341 [==============================] - 26s 75ms/step - loss: 1.3479\n",
            "Epoch 14/200\n",
            "341/341 [==============================] - 26s 75ms/step - loss: 1.3360\n",
            "Epoch 15/200\n",
            "341/341 [==============================] - 26s 75ms/step - loss: 1.3228\n",
            "Epoch 16/200\n",
            "341/341 [==============================] - 26s 75ms/step - loss: 1.3124\n",
            "Epoch 17/200\n",
            "341/341 [==============================] - 26s 75ms/step - loss: 1.3015\n",
            "Epoch 18/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 1.2917\n",
            "Epoch 19/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 1.2825\n",
            "Epoch 20/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 1.2729\n",
            "Epoch 21/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 1.2644\n",
            "Epoch 22/200\n",
            "341/341 [==============================] - 26s 75ms/step - loss: 1.2553\n",
            "Epoch 23/200\n",
            "341/341 [==============================] - 27s 75ms/step - loss: 1.2472\n",
            "Epoch 24/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 1.2412\n",
            "Epoch 25/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 1.2333\n",
            "Epoch 26/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 1.2250\n",
            "Epoch 27/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 1.2178\n",
            "Epoch 28/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 1.2122\n",
            "Epoch 29/200\n",
            "341/341 [==============================] - 26s 75ms/step - loss: 1.2049\n",
            "Epoch 30/200\n",
            "341/341 [==============================] - 26s 75ms/step - loss: 1.1990\n",
            "Epoch 31/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 1.1947\n",
            "Epoch 32/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 1.1889\n",
            "Epoch 33/200\n",
            "341/341 [==============================] - 26s 75ms/step - loss: 1.1840\n",
            "Epoch 34/200\n",
            "341/341 [==============================] - 27s 75ms/step - loss: 1.1803\n",
            "Epoch 35/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 1.1762\n",
            "Epoch 36/200\n",
            "341/341 [==============================] - 27s 75ms/step - loss: 1.1719\n",
            "Epoch 37/200\n",
            "341/341 [==============================] - 26s 75ms/step - loss: 1.1680\n",
            "Epoch 38/200\n",
            "341/341 [==============================] - 27s 75ms/step - loss: 1.1647\n",
            "Epoch 39/200\n",
            "341/341 [==============================] - 27s 75ms/step - loss: 1.1621\n",
            "Epoch 40/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 1.1587\n",
            "Epoch 41/200\n",
            "341/341 [==============================] - 27s 75ms/step - loss: 1.1560\n",
            "Epoch 42/200\n",
            "341/341 [==============================] - 27s 75ms/step - loss: 1.1540\n",
            "Epoch 43/200\n",
            "341/341 [==============================] - 26s 75ms/step - loss: 1.1536\n",
            "Epoch 44/200\n",
            "341/341 [==============================] - 26s 74ms/step - loss: 1.1500\n",
            "Epoch 45/200\n",
            "341/341 [==============================] - 26s 74ms/step - loss: 1.1490\n",
            "Epoch 46/200\n",
            "341/341 [==============================] - 26s 74ms/step - loss: 1.1487\n",
            "Epoch 47/200\n",
            "341/341 [==============================] - 26s 74ms/step - loss: 1.1464\n",
            "Epoch 48/200\n",
            "341/341 [==============================] - 26s 74ms/step - loss: 1.1472\n",
            "Epoch 49/200\n",
            "341/341 [==============================] - 26s 74ms/step - loss: 1.1462\n",
            "Epoch 50/200\n",
            "341/341 [==============================] - 26s 74ms/step - loss: 1.1462\n",
            "Epoch 51/200\n",
            "341/341 [==============================] - 26s 73ms/step - loss: 1.1446\n",
            "Epoch 52/200\n",
            "341/341 [==============================] - 26s 74ms/step - loss: 1.1453\n",
            "Epoch 53/200\n",
            "341/341 [==============================] - 26s 74ms/step - loss: 1.1474\n",
            "Epoch 54/200\n",
            "341/341 [==============================] - 26s 75ms/step - loss: 1.1464\n",
            "Epoch 55/200\n",
            "341/341 [==============================] - 26s 74ms/step - loss: 1.1474\n",
            "Epoch 56/200\n",
            "341/341 [==============================] - 26s 75ms/step - loss: 1.1492\n",
            "\n",
            "Epoch 00056: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "Epoch 57/200\n",
            "341/341 [==============================] - 26s 74ms/step - loss: 1.0384\n",
            "Epoch 58/200\n",
            "341/341 [==============================] - 26s 75ms/step - loss: 0.9998\n",
            "Epoch 59/200\n",
            "341/341 [==============================] - 26s 75ms/step - loss: 0.9830\n",
            "Epoch 60/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.9697\n",
            "Epoch 61/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.9587\n",
            "Epoch 62/200\n",
            "341/341 [==============================] - 27s 75ms/step - loss: 0.9493\n",
            "Epoch 63/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.9400\n",
            "Epoch 64/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.9311\n",
            "Epoch 65/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.9231\n",
            "Epoch 66/200\n",
            "341/341 [==============================] - 26s 75ms/step - loss: 0.9153\n",
            "Epoch 67/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.9076\n",
            "Epoch 68/200\n",
            "341/341 [==============================] - 26s 75ms/step - loss: 0.9002\n",
            "Epoch 69/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.8933\n",
            "Epoch 70/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.8869\n",
            "Epoch 71/200\n",
            "341/341 [==============================] - 26s 75ms/step - loss: 0.8806\n",
            "Epoch 72/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.8743\n",
            "Epoch 73/200\n",
            "341/341 [==============================] - 26s 75ms/step - loss: 0.8686\n",
            "Epoch 74/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.8624\n",
            "Epoch 75/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.8568\n",
            "Epoch 76/200\n",
            "341/341 [==============================] - 27s 75ms/step - loss: 0.8522\n",
            "Epoch 77/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.8462\n",
            "Epoch 78/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.8418\n",
            "Epoch 79/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.8366\n",
            "Epoch 80/200\n",
            "341/341 [==============================] - 26s 75ms/step - loss: 0.8312\n",
            "Epoch 81/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.8270\n",
            "Epoch 82/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.8221\n",
            "Epoch 83/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.8187\n",
            "Epoch 84/200\n",
            "341/341 [==============================] - 27s 75ms/step - loss: 0.8141\n",
            "Epoch 85/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.8099\n",
            "Epoch 86/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.8061\n",
            "Epoch 87/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.8017\n",
            "Epoch 88/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.7979\n",
            "Epoch 89/200\n",
            "341/341 [==============================] - 26s 75ms/step - loss: 0.7942\n",
            "Epoch 90/200\n",
            "341/341 [==============================] - 27s 75ms/step - loss: 0.7910\n",
            "Epoch 91/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.7877\n",
            "Epoch 92/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.7838\n",
            "Epoch 93/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.7806\n",
            "Epoch 94/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.7782\n",
            "Epoch 95/200\n",
            "341/341 [==============================] - 27s 75ms/step - loss: 0.7745\n",
            "Epoch 96/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.7709\n",
            "Epoch 97/200\n",
            "341/341 [==============================] - 26s 75ms/step - loss: 0.7676\n",
            "Epoch 98/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.7652\n",
            "Epoch 99/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.7623\n",
            "Epoch 100/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.7583\n",
            "Epoch 101/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.7570\n",
            "Epoch 102/200\n",
            "341/341 [==============================] - 27s 77ms/step - loss: 0.7545\n",
            "Epoch 103/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.7517\n",
            "Epoch 104/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.7490\n",
            "Epoch 105/200\n",
            "341/341 [==============================] - 27s 77ms/step - loss: 0.7456\n",
            "Epoch 106/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.7433\n",
            "Epoch 107/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.7407\n",
            "Epoch 108/200\n",
            "341/341 [==============================] - 27s 77ms/step - loss: 0.7383\n",
            "Epoch 109/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.7362\n",
            "Epoch 110/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.7338\n",
            "Epoch 111/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.7329\n",
            "Epoch 112/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.7295\n",
            "Epoch 113/200\n",
            "341/341 [==============================] - 27s 77ms/step - loss: 0.7279\n",
            "Epoch 114/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.7259\n",
            "Epoch 115/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.7239\n",
            "Epoch 116/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.7220\n",
            "Epoch 117/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.7193\n",
            "Epoch 118/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.7186\n",
            "Epoch 119/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.7159\n",
            "Epoch 120/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.7138\n",
            "Epoch 121/200\n",
            "341/341 [==============================] - 26s 75ms/step - loss: 0.7120\n",
            "Epoch 122/200\n",
            "341/341 [==============================] - 26s 75ms/step - loss: 0.7102\n",
            "Epoch 123/200\n",
            "341/341 [==============================] - 26s 75ms/step - loss: 0.7069\n",
            "Epoch 124/200\n",
            "341/341 [==============================] - 26s 75ms/step - loss: 0.7069\n",
            "Epoch 125/200\n",
            "341/341 [==============================] - 26s 75ms/step - loss: 0.7053\n",
            "Epoch 126/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.7049\n",
            "Epoch 127/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.7021\n",
            "Epoch 128/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.7008\n",
            "Epoch 129/200\n",
            "341/341 [==============================] - 27s 75ms/step - loss: 0.6987\n",
            "Epoch 130/200\n",
            "341/341 [==============================] - 26s 75ms/step - loss: 0.6983\n",
            "Epoch 131/200\n",
            "341/341 [==============================] - 27s 75ms/step - loss: 0.6952\n",
            "Epoch 132/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.6942\n",
            "Epoch 133/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.6921\n",
            "Epoch 134/200\n",
            "341/341 [==============================] - 26s 75ms/step - loss: 0.6919\n",
            "Epoch 135/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.6909\n",
            "Epoch 136/200\n",
            "341/341 [==============================] - 27s 77ms/step - loss: 0.6878\n",
            "Epoch 137/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.6873\n",
            "Epoch 138/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.6858\n",
            "Epoch 139/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.6847\n",
            "Epoch 140/200\n",
            "341/341 [==============================] - 27s 77ms/step - loss: 0.6848\n",
            "Epoch 141/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.6825\n",
            "Epoch 142/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.6787\n",
            "Epoch 143/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.6796\n",
            "Epoch 144/200\n",
            "341/341 [==============================] - 27s 77ms/step - loss: 0.6778\n",
            "Epoch 145/200\n",
            "341/341 [==============================] - 27s 77ms/step - loss: 0.6765\n",
            "Epoch 146/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.6776\n",
            "Epoch 147/200\n",
            "341/341 [==============================] - 27s 77ms/step - loss: 0.6759\n",
            "Epoch 148/200\n",
            "341/341 [==============================] - 27s 77ms/step - loss: 0.6737\n",
            "Epoch 149/200\n",
            "341/341 [==============================] - 27s 77ms/step - loss: 0.6726\n",
            "Epoch 150/200\n",
            "341/341 [==============================] - 27s 77ms/step - loss: 0.6707\n",
            "Epoch 151/200\n",
            "341/341 [==============================] - 27s 77ms/step - loss: 0.6711\n",
            "Epoch 152/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.6692\n",
            "Epoch 153/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.6676\n",
            "Epoch 154/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.6676\n",
            "Epoch 155/200\n",
            "341/341 [==============================] - 27s 77ms/step - loss: 0.6646\n",
            "Epoch 156/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.6653\n",
            "Epoch 157/200\n",
            "341/341 [==============================] - 27s 77ms/step - loss: 0.6647\n",
            "Epoch 158/200\n",
            "341/341 [==============================] - 27s 77ms/step - loss: 0.6625\n",
            "Epoch 159/200\n",
            "341/341 [==============================] - 27s 77ms/step - loss: 0.6612\n",
            "Epoch 160/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.6635\n",
            "Epoch 161/200\n",
            "341/341 [==============================] - 27s 77ms/step - loss: 0.6598\n",
            "Epoch 162/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.6599\n",
            "Epoch 163/200\n",
            "341/341 [==============================] - 27s 77ms/step - loss: 0.6602\n",
            "Epoch 164/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.6581\n",
            "Epoch 165/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.6565\n",
            "Epoch 166/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.6541\n",
            "Epoch 167/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.6532\n",
            "Epoch 168/200\n",
            "341/341 [==============================] - 27s 77ms/step - loss: 0.6560\n",
            "Epoch 169/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.6527\n",
            "Epoch 170/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.6533\n",
            "Epoch 171/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.6507\n",
            "Epoch 172/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.6541\n",
            "Epoch 173/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.6510\n",
            "Epoch 174/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.6466\n",
            "Epoch 175/200\n",
            "341/341 [==============================] - 27s 77ms/step - loss: 0.6475\n",
            "Epoch 176/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.6484\n",
            "Epoch 177/200\n",
            "341/341 [==============================] - 27s 77ms/step - loss: 0.6475\n",
            "Epoch 178/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.6477\n",
            "Epoch 179/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.6468\n",
            "\n",
            "Epoch 00179: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "Epoch 180/200\n",
            "341/341 [==============================] - 27s 77ms/step - loss: 0.5904\n",
            "Epoch 181/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.5706\n",
            "Epoch 182/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.5634\n",
            "Epoch 183/200\n",
            "341/341 [==============================] - 26s 75ms/step - loss: 0.5586\n",
            "Epoch 184/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.5552\n",
            "Epoch 185/200\n",
            "341/341 [==============================] - 27s 77ms/step - loss: 0.5526\n",
            "Epoch 186/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.5503\n",
            "Epoch 187/200\n",
            "341/341 [==============================] - 27s 77ms/step - loss: 0.5483\n",
            "Epoch 188/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.5465\n",
            "Epoch 189/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.5453\n",
            "Epoch 190/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.5432\n",
            "Epoch 191/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.5415\n",
            "Epoch 192/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.5401\n",
            "Epoch 193/200\n",
            "341/341 [==============================] - 27s 77ms/step - loss: 0.5386\n",
            "Epoch 194/200\n",
            "341/341 [==============================] - 27s 77ms/step - loss: 0.5373\n",
            "Epoch 195/200\n",
            "341/341 [==============================] - 27s 77ms/step - loss: 0.5361\n",
            "Epoch 196/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.5348\n",
            "Epoch 197/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.5336\n",
            "Epoch 198/200\n",
            "341/341 [==============================] - 27s 77ms/step - loss: 0.5327\n",
            "Epoch 199/200\n",
            "341/341 [==============================] - 27s 77ms/step - loss: 0.5312\n",
            "Epoch 200/200\n",
            "341/341 [==============================] - 27s 76ms/step - loss: 0.5304\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "Bj32PyE8e3Pu",
        "outputId": "9ea628ee-6c49-4a08-ce5b-6a5b97f675ad"
      },
      "source": [
        "loss_plot(history_rnn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_i3VQbgcXLe"
      },
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    return predicted_chars, states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pP1aXvlcdMo"
      },
      "source": [
        "one_step_model = OneStep(rnn_model, chars_from_ids, ids_from_chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtR4NKAXcdJN",
        "outputId": "2f6cd4bd-c25a-4012-9719-1ad8d2cd15ba"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Romeo?\n",
            "\n",
            "MERCUTIO:\n",
            "My lords\n",
            "are at the place,--\n",
            "\n",
            "MENENIUS:\n",
            "O my liege,\n",
            "Heart love aurable and by himself.\n",
            "\n",
            "WARWICK:\n",
            "And thou at their country are o' the people.\n",
            "\n",
            "Pursuivant:\n",
            "I thought your name is Duke of Norfolk lenity\n",
            "to prepare myself.\n",
            "\n",
            "Nurse:\n",
            "Marry, I'll report it, adieu; and most desires my uncle Retemosing.\n",
            "\n",
            "SICINIUS:\n",
            "'Twas no thing, I say the price o' the sly government,\n",
            "Yet laugh at time right.\n",
            "\n",
            "LEONTES:\n",
            "How! can yield me not, my lord, and you mistakes discreetly hates of pity.\n",
            "The Lords\n",
            "not to me this shall proceed\n",
            "For thy head by his head to prepare my friends as that with his grace,\n",
            "To catch my duty!\n",
            "\n",
            "Second Keeper:\n",
            "Besides, you\n",
            "A strange fit to do,\n",
            "By lack of love and roteer.\n",
            "A wickion too, for this night.\n",
            "\n",
            "PETRUCHIO:\n",
            "Why, hence!\n",
            "\n",
            "AUFIDIUS:\n",
            "Go tell thee:\n",
            "He that both of your office that's gone.\n",
            "\n",
            "Clown:\n",
            "We are inform'd the stuff,\n",
            "From which I dourt-port with Proteanue; for thou hast been!\n",
            "\n",
            "ANGELO:\n",
            "What store were they shall compell'd\n",
            "As to your mother\n",
            "That white Richard reign from \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.64516019821167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_VUxZ-Nccvr",
        "outputId": "84c5d200-5ec4-4180-d33d-18ce6662d698"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b\"Romeo\\nTo take order we saw now, this land will happy foolery,\\nBeing one craves a near\\nThat runagate guest: here is here unto this kern'd?\\nWhat is the provost? What, marvel, spend thee to the city.\\nIs cries King Edward Edward Plainnal tears,\\nAnd love thee o'er in smiles of quality\\nHis ars. Come, let's be;\\nAnd must go with me for Rome again after a great deal to tell it not.\\n\\nSecond Senator:\\nCurse of remorse; his name remain\\nA dozen out of.\\n\\nELBOW:\\nAy, sir; I have\\nThe prince I came\\nTo liberty.\\nUnless for my own,--as the queen through these woes,\\nAnd am I sent for\\nthis good with a wealth\\nShould be county is grown but Henry is the bastard.\\n\\nBIANCA:\\nThat 'dive, it must needs be dine;\\nHave chip my father well.\\n\\nLUCIO:\\nHe was dream i' the commons which is another fellow'd all this,\\nI have some more from Green, some other side more.\\nFor what he, to our gentlemen,\\nTherefore yet hath set his shriving: why stand up:'l dishonour, in good hope to worn number issue,\\nAnd craves you to his claporate you,\\nI\"\n",
            " b\"Romeo slew him that.\\n\\nFirst Gentleman:\\nI know what thou hast thy right,\\nTo be discover'd.\\n\\nSecond Citizen:\\nAnd so 'twere allayed my death,\\nAnd all my brother's way:\\nSo many good morrows the place; thou likeness of thine own\\nWith words are mad:--an they of his throne?\\n\\nLEONTES:\\nGood night, 'Sicistant: one thing she will witn me and on the prison: restorments do babb. They can say you told you in my dishonour heaving, perjured Gaunt appear before Eichmond or himself amongst the currips you have leave it array;\\nWhich, lead all discourse,\\nWho should infringe the corse; or, those pity, fond murdering tongues: let me die the wenting women or two?\\nIs the most needful\\ncannot be affection.\\n\\nLUCENTIO:\\nI fear it is: what news?\\n\\nLEONTES:\\nGo, get thee to charity will ride us all into an oath by Him,\\nThat makes his way?\\nI take the truth, all butchered;\\nNor both ere mistress waves\\nThat e'er to me inform\\nStands a little find\\nUpon as Hereford's son: his nature\\nWere in hell,\\nThe leaves and counterfeit'st odi\"\n",
            " b\"Romeo,\\nWhose dearhed in Russil with him\\nAnd from this time of day!\\n\\nPERDITA:\\nYour tribunes; they shall do come and long-sings it.\\n\\nSecond Servingman:\\nWhy, is it not enter!\\n\\nSICINIUS:\\nNay, I might be your grace I might flatter first conscience is wash of our time:\\nThe daughter, fills our commission, and discourse,\\nWhence are you?\\n\\nROMEO:\\nThou canst go to-night.\\n\\nROMEO:\\nThough he did, and cannot list.\\n\\nVINCENTIO:\\nAnd you my good lord and villain: for, every wonder hope that you do it!\\n\\nESCALUS:\\nThen she is my son; it is not infusion; you'll keep at Halfor Peter-song forth,\\nAnd many gentlemen! I have said against the stars\\nOf eyes,\\nElse, for your company, set on counsel.\\nDo so, how then, if ever true like beast!\\nLook her, I'ld they were fled to thee at thy good soul\\nIs to bite heir to the blind,\\nLet me embrace with the joint-stripes\\nIs nothing but convenience, and every thing ill-siste I did infer thee in.\\nWhen with five thou shalt tell the ground.\\n\\nKING RICHARD III:\\nFarewell, for thy name?\\n\\nC\"\n",
            " b\"Romeo:\\nInprobeth no single-solking: if I then dangerous\\nthe thoughts, that Adrags of your confines of her who,\\nWas it not stay the vessel\\nTo aved to laugh-eleven of truth, but bless you, as hardly will to Aufidius are:\\nThese are this?\\n\\nMENENIUS:\\nBound with Rome will be deliver'd by,\\nHe could please you, sir.\\n\\nHORTENSIO:\\nSaw'st thou not thy sides\\nthat nothing but a prince, and thou with the king hath brought it over\\nTo execute the gown I endown to see the world\\nLeave before hungerly and a heavy spider\\nAnd promise them,\\nsweet disfignty;\\nTo beg of his atteman:\\nArt thou old match,\\nPut the rocks be pardon'd, or do I see thy franging floods and\\nthe treason with my tent thereof,\\nAnd in hope of tongues; and I am sure\\nof all that I see a tawards\\nThe wisdom was it not know--door father.\\n\\nPOMPEY:\\nSir, I have done: 'tis twenty heal, should know\\nWhat's yet\\nAcquainted in the action of death.\\n\\nGREMIO:\\nNo, at the Duke of Gloucester;\\nFor the rest was but thought of mine\\nCar I am sure, but that\\nWhich gain in\"\n",
            " b\"Romeo\\nDid murdering of his surly glanced\\nHe thinks the spur;\\nNow are these confines ne'er buzzards.\\n\\nMENENIUS:\\nI'll swear,--your worship is not honest.\\n\\nDUKE OF AUMERLE:\\n'Tis figured in me; and, Camillo, and\\nyou not their purpose bids us 'twixt thy beggar\\nAnd never to my guard.\\n\\nKING HENRY VI:\\nSaddle and to make way, as I am answer when it shall\\nbut peace is nothing: there are you,\\nAnd first begin with thee\\nWhere he shall stay\\nTitly all at once,\\nOf my life, for it is no remedy.\\n\\nGLOUCESTER:\\nTake the wack him up in hunder soldier\\nOf your eyes fast,\\nSo yourging the worst o' the chosen of thy faults. Is quick conditions and twenty or here!\\nIs me! here, sir. Fare you so hot, but since I stay, the king hath yet\\nmethinks you to offerrips of love and lock'd at all time I mean, my lord.\\n\\nHENRY BOLINGBROKE:\\nMarry, wilt thou be overture\\n'Twere well to employ'd with rash-levish earth,\\nLet's first he do remain:\\nYour worship is deceive\\nWe marvel must be burnt for his friends,--\\n\\nESCALUS:\\nDream, come.\\n\\nC\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.0402941703796387\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-32d_gikZGod"
      },
      "source": [
        "# GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3vaL8ogswuk"
      },
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   activation='tanh',\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True\n",
        "                                   )\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x,states\n",
        "    else:\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOR6di_uwQba"
      },
      "source": [
        "gru_model = MyModel(\n",
        "    vocab_size = len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_tnsqp5W0JD",
        "outputId": "51b50540-e548-47f7-ed4c-c34e8f77b73c"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_preds = gru_model(input_example_batch)\n",
        "  print(example_batch_preds.shape, '#(batch_size, sequence_length, vocab_size)')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(64, 50, 66) #(batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kMZLYx1wsyG",
        "outputId": "bcc4d26d-3233-4506-c668-6915d9480581"
      },
      "source": [
        "gru_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"my_model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      multiple                  16896     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    multiple                  3938304   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              multiple                  67650     \n",
            "=================================================================\n",
            "Total params: 4,022,850\n",
            "Trainable params: 4,022,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6Xvnr8OW4Nb"
      },
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAkpikZiW7A4"
      },
      "source": [
        "opt = keras.optimizers.RMSprop(learning_rate=1e-3)\n",
        "gru_model.compile(optimizer=opt, loss=loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ut1FseAfzTKj"
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True,\n",
        "    save_freq = 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VY6N_BLFzYY-"
      },
      "source": [
        "epochs = 200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BxXEuzozaT_",
        "outputId": "77498332-a760-4731-b5fb-20f59fce40a6"
      },
      "source": [
        "early_stopping = EarlyStopping(monitor = 'loss', min_delta=0.001, patience = 10, mode = 'min', verbose=2)\n",
        "reduce_lr =  ReduceLROnPlateau(monitor='loss', factor=0.2,\n",
        "                              patience=5, min_lr=0.000001, verbose=2)\n",
        "history_gru = gru_model.fit(dataset, epochs=epochs, callbacks=[early_stopping, reduce_lr, checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "341/341 [==============================] - 27s 72ms/step - loss: 2.2346\n",
            "Epoch 2/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 1.6333\n",
            "Epoch 3/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 1.4683\n",
            "Epoch 4/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 1.3819\n",
            "Epoch 5/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 1.3191\n",
            "Epoch 6/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 1.2660\n",
            "Epoch 7/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 1.2144\n",
            "Epoch 8/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 1.1630\n",
            "Epoch 9/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 1.1101\n",
            "Epoch 10/200\n",
            "341/341 [==============================] - 25s 71ms/step - loss: 1.0554\n",
            "Epoch 11/200\n",
            "341/341 [==============================] - 25s 71ms/step - loss: 0.9998\n",
            "Epoch 12/200\n",
            "341/341 [==============================] - 25s 71ms/step - loss: 0.9438\n",
            "Epoch 13/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.8888\n",
            "Epoch 14/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.8360\n",
            "Epoch 15/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.7869\n",
            "Epoch 16/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.7429\n",
            "Epoch 17/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.7048\n",
            "Epoch 18/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.6714\n",
            "Epoch 19/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.6434\n",
            "Epoch 20/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.6201\n",
            "Epoch 21/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.6008\n",
            "Epoch 22/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.5857\n",
            "Epoch 23/200\n",
            "341/341 [==============================] - 25s 71ms/step - loss: 0.5727\n",
            "Epoch 24/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.5630\n",
            "Epoch 25/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.5558\n",
            "Epoch 26/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.5504\n",
            "Epoch 27/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.5461\n",
            "Epoch 28/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.5437\n",
            "Epoch 29/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.5416\n",
            "Epoch 30/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.5410\n",
            "Epoch 31/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.5401\n",
            "Epoch 32/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.5408\n",
            "Epoch 33/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.5424\n",
            "Epoch 34/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.5438\n",
            "Epoch 35/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.5460\n",
            "Epoch 36/200\n",
            "341/341 [==============================] - 26s 73ms/step - loss: 0.5483\n",
            "\n",
            "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "Epoch 37/200\n",
            "341/341 [==============================] - 26s 73ms/step - loss: 0.4026\n",
            "Epoch 38/200\n",
            "341/341 [==============================] - 26s 73ms/step - loss: 0.3193\n",
            "Epoch 39/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.2838\n",
            "Epoch 40/200\n",
            "341/341 [==============================] - 26s 73ms/step - loss: 0.2626\n",
            "Epoch 41/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.2474\n",
            "Epoch 42/200\n",
            "341/341 [==============================] - 26s 72ms/step - loss: 0.2361\n",
            "Epoch 43/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.2272\n",
            "Epoch 44/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.2200\n",
            "Epoch 45/200\n",
            "341/341 [==============================] - 25s 71ms/step - loss: 0.2139\n",
            "Epoch 46/200\n",
            "341/341 [==============================] - 25s 71ms/step - loss: 0.2090\n",
            "Epoch 47/200\n",
            "341/341 [==============================] - 25s 71ms/step - loss: 0.2047\n",
            "Epoch 48/200\n",
            "341/341 [==============================] - 25s 71ms/step - loss: 0.2010\n",
            "Epoch 49/200\n",
            "341/341 [==============================] - 25s 71ms/step - loss: 0.1976\n",
            "Epoch 50/200\n",
            "341/341 [==============================] - 25s 71ms/step - loss: 0.1949\n",
            "Epoch 51/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.1925\n",
            "Epoch 52/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.1904\n",
            "Epoch 53/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.1884\n",
            "Epoch 54/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.1867\n",
            "Epoch 55/200\n",
            "341/341 [==============================] - 25s 71ms/step - loss: 0.1851\n",
            "Epoch 56/200\n",
            "341/341 [==============================] - 25s 71ms/step - loss: 0.1837\n",
            "Epoch 57/200\n",
            "341/341 [==============================] - 25s 71ms/step - loss: 0.1823\n",
            "Epoch 58/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.1813\n",
            "Epoch 59/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.1801\n",
            "Epoch 60/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.1793\n",
            "Epoch 61/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.1783\n",
            "Epoch 62/200\n",
            "341/341 [==============================] - 25s 71ms/step - loss: 0.1773\n",
            "Epoch 63/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.1765\n",
            "Epoch 64/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.1760\n",
            "Epoch 65/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.1752\n",
            "Epoch 66/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.1746\n",
            "Epoch 67/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.1742\n",
            "Epoch 68/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.1735\n",
            "Epoch 69/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.1730\n",
            "Epoch 70/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.1726\n",
            "Epoch 71/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.1720\n",
            "Epoch 72/200\n",
            "341/341 [==============================] - 26s 73ms/step - loss: 0.1717\n",
            "Epoch 73/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.1711\n",
            "Epoch 74/200\n",
            "341/341 [==============================] - 25s 73ms/step - loss: 0.1708\n",
            "Epoch 75/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.1705\n",
            "Epoch 76/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.1703\n",
            "Epoch 77/200\n",
            "341/341 [==============================] - 26s 72ms/step - loss: 0.1699\n",
            "Epoch 78/200\n",
            "341/341 [==============================] - 26s 73ms/step - loss: 0.1696\n",
            "Epoch 79/200\n",
            "341/341 [==============================] - 25s 72ms/step - loss: 0.1693\n",
            "Epoch 80/200\n",
            "341/341 [==============================] - 26s 73ms/step - loss: 0.1690\n",
            "Epoch 81/200\n",
            "341/341 [==============================] - 25s 71ms/step - loss: 0.1686\n",
            "Epoch 82/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1685\n",
            "Epoch 83/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1682\n",
            "Epoch 84/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1680\n",
            "Epoch 85/200\n",
            "341/341 [==============================] - 25s 71ms/step - loss: 0.1676\n",
            "Epoch 86/200\n",
            "341/341 [==============================] - 25s 71ms/step - loss: 0.1675\n",
            "Epoch 87/200\n",
            "341/341 [==============================] - 25s 71ms/step - loss: 0.1673\n",
            "Epoch 88/200\n",
            "341/341 [==============================] - 25s 71ms/step - loss: 0.1672\n",
            "Epoch 89/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1670\n",
            "Epoch 90/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1668\n",
            "Epoch 91/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1665\n",
            "Epoch 92/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1663\n",
            "Epoch 93/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1663\n",
            "Epoch 94/200\n",
            "341/341 [==============================] - 25s 71ms/step - loss: 0.1660\n",
            "Epoch 95/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1659\n",
            "Epoch 96/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1658\n",
            "Epoch 97/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1656\n",
            "Epoch 98/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1654\n",
            "Epoch 99/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1654\n",
            "Epoch 100/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1652\n",
            "Epoch 101/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1651\n",
            "Epoch 102/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1648\n",
            "Epoch 103/200\n",
            "341/341 [==============================] - 25s 71ms/step - loss: 0.1648\n",
            "Epoch 104/200\n",
            "341/341 [==============================] - 25s 71ms/step - loss: 0.1647\n",
            "Epoch 105/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1644\n",
            "Epoch 106/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1645\n",
            "Epoch 107/200\n",
            "341/341 [==============================] - 24s 69ms/step - loss: 0.1644\n",
            "Epoch 108/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1644\n",
            "Epoch 109/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1641\n",
            "Epoch 110/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1641\n",
            "Epoch 111/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1640\n",
            "Epoch 112/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1639\n",
            "Epoch 113/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1637\n",
            "Epoch 114/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1637\n",
            "Epoch 115/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1636\n",
            "Epoch 116/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1636\n",
            "Epoch 117/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1634\n",
            "Epoch 118/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1633\n",
            "Epoch 119/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1634\n",
            "Epoch 120/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1631\n",
            "Epoch 121/200\n",
            "341/341 [==============================] - 25s 71ms/step - loss: 0.1630\n",
            "Epoch 122/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1631\n",
            "Epoch 123/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1631\n",
            "Epoch 124/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1629\n",
            "Epoch 125/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1629\n",
            "Epoch 126/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1627\n",
            "Epoch 127/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1627\n",
            "Epoch 128/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1626\n",
            "Epoch 129/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1626\n",
            "Epoch 130/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1623\n",
            "Epoch 131/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1624\n",
            "Epoch 132/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1623\n",
            "Epoch 133/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1623\n",
            "Epoch 134/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1622\n",
            "Epoch 135/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1622\n",
            "Epoch 136/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1622\n",
            "Epoch 137/200\n",
            "341/341 [==============================] - 25s 71ms/step - loss: 0.1621\n",
            "Epoch 138/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1619\n",
            "Epoch 139/200\n",
            "341/341 [==============================] - 24s 69ms/step - loss: 0.1620\n",
            "Epoch 140/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1618\n",
            "Epoch 141/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1620\n",
            "Epoch 142/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1617\n",
            "Epoch 143/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1618\n",
            "Epoch 144/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1617\n",
            "Epoch 145/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1617\n",
            "Epoch 146/200\n",
            "341/341 [==============================] - 25s 71ms/step - loss: 0.1616\n",
            "Epoch 147/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1615\n",
            "Epoch 148/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1616\n",
            "Epoch 149/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1616\n",
            "Epoch 150/200\n",
            "341/341 [==============================] - 25s 71ms/step - loss: 0.1615\n",
            "Epoch 151/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1612\n",
            "Epoch 152/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1615\n",
            "Epoch 153/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1612\n",
            "Epoch 154/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1612\n",
            "Epoch 155/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1612\n",
            "Epoch 156/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1610\n",
            "Epoch 157/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1612\n",
            "Epoch 158/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1611\n",
            "Epoch 159/200\n",
            "341/341 [==============================] - 25s 71ms/step - loss: 0.1611\n",
            "Epoch 160/200\n",
            "341/341 [==============================] - 25s 71ms/step - loss: 0.1609\n",
            "Epoch 161/200\n",
            "341/341 [==============================] - 25s 71ms/step - loss: 0.1608\n",
            "Epoch 162/200\n",
            "341/341 [==============================] - 25s 71ms/step - loss: 0.1609\n",
            "Epoch 163/200\n",
            "341/341 [==============================] - 25s 71ms/step - loss: 0.1609\n",
            "Epoch 164/200\n",
            "341/341 [==============================] - 25s 71ms/step - loss: 0.1608\n",
            "Epoch 165/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1607\n",
            "Epoch 166/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1606\n",
            "Epoch 167/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1607\n",
            "Epoch 168/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1605\n",
            "Epoch 169/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1605\n",
            "Epoch 170/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1607\n",
            "Epoch 171/200\n",
            "341/341 [==============================] - 25s 71ms/step - loss: 0.1606\n",
            "Epoch 172/200\n",
            "341/341 [==============================] - 25s 71ms/step - loss: 0.1606\n",
            "Epoch 173/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1606\n",
            "\n",
            "Epoch 00173: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "Epoch 174/200\n",
            "341/341 [==============================] - 25s 71ms/step - loss: 0.1485\n",
            "Epoch 175/200\n",
            "341/341 [==============================] - 25s 71ms/step - loss: 0.1454\n",
            "Epoch 176/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1446\n",
            "Epoch 177/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1442\n",
            "Epoch 178/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1440\n",
            "Epoch 179/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1438\n",
            "Epoch 180/200\n",
            "341/341 [==============================] - 25s 71ms/step - loss: 0.1437\n",
            "Epoch 181/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1437\n",
            "Epoch 182/200\n",
            "341/341 [==============================] - 25s 71ms/step - loss: 0.1435\n",
            "Epoch 183/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1435\n",
            "Epoch 184/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1434\n",
            "Epoch 185/200\n",
            "341/341 [==============================] - 25s 71ms/step - loss: 0.1433\n",
            "Epoch 186/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1433\n",
            "Epoch 187/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1432\n",
            "Epoch 188/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1432\n",
            "Epoch 189/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1432\n",
            "Epoch 190/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1431\n",
            "Epoch 191/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1431\n",
            "Epoch 192/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1430\n",
            "Epoch 193/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1430\n",
            "Epoch 194/200\n",
            "341/341 [==============================] - 25s 71ms/step - loss: 0.1430\n",
            "Epoch 195/200\n",
            "341/341 [==============================] - 25s 71ms/step - loss: 0.1430\n",
            "Epoch 196/200\n",
            "341/341 [==============================] - 25s 71ms/step - loss: 0.1429\n",
            "Epoch 197/200\n",
            "341/341 [==============================] - 25s 71ms/step - loss: 0.1429\n",
            "Epoch 198/200\n",
            "341/341 [==============================] - 25s 71ms/step - loss: 0.1429\n",
            "Epoch 199/200\n",
            "341/341 [==============================] - 25s 71ms/step - loss: 0.1428\n",
            "Epoch 200/200\n",
            "341/341 [==============================] - 25s 70ms/step - loss: 0.1428\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WJTlZ0n9e-Pp",
        "outputId": "f0d73a54-94c1-4455-95f4-502077563fc7"
      },
      "source": [
        "loss_plot(history_gru)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qQahSvT6zdDu"
      },
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    return predicted_chars, states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ls0cxWg54uWG"
      },
      "source": [
        "one_step_model = OneStep(gru_model, chars_from_ids, ids_from_chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bIFfsg4j7bPE",
        "outputId": "ebdcf938-86c9-4ab3-edd1-e3d3277321e1"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Romeo\n",
            "To comfort you: I wot well where he is.\n",
            "Hark you, sir, be gone, conduit that the absent duke.\n",
            "\n",
            "LUCIO:\n",
            "A very superficial, ignorant, any thing, my gracious lord.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "Nay, then his good hearts and think some would have deadly shape.\n",
            "\n",
            "LEONTES:\n",
            "By my head with such mind he bear me: I hope, and will\n",
            "Prithee now, sweet father, dissal time, and snaples;\n",
            "Where were her life? she durst not call me.'\n",
            "\n",
            "Third Citizen:\n",
            "Nurse, come hither, master and my father?\n",
            "\n",
            "POMPEY:\n",
            "They shall stir a virtuous strength constant me.\n",
            "I hope the king miscarried him where he is\n",
            "aboard, tender your intercepantion hablest from the facation\n",
            "A poor and brook itted story of the feast,\n",
            "And not our tenret till away him speak for the other senses. I cannot fly.\n",
            "\n",
            "Ahe heaven with those that should have laid our friends, ready in your commony.\n",
            "\n",
            "CAMILLO:\n",
            "I am afray: but hear me.\n",
            "Your breast for fair cousin Hereford and the fear\n",
            "Would sleep that women are so storm, else, when a country, as those colours\n",
            "Not pale as \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.999271869659424\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3JN6S4887ePp",
        "outputId": "2d244b6c-a913-4163-f068-53a963154277"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b\"Romeo,\\nWho had but newly entertain'd revenge,\\nAnd the subjects to infringed visits,\\nHe till the end of the horses; in the consecrate of his troop's deep,\\nThat bad tirtGor Jamy of the same I may condect what we in placeed.\\nLord, what says he of our friendshorped.\\nWhere is my lady widow sister?\\n\\nPETRUCHIO:\\nWhat, shall this speech be Clifford, in Were well.\\nThere is the womb for wronged woe.\\n\\nGREMIO:\\nBeloved mortality, that thou canst undernearth.\\n\\nQUEEN MARGARET:\\nRichard!\\n\\nEDWARD:\\nLet Geppoot that raison rags and treasure prove as he\\npies in God's name, shall to this prayers; for one things loved\\nWhich made you go to the crown and only clap their backs.\\nShe hap of revolter, father, do more unnatural: soetly men, and where\\nIf she hath a power on her to question my misery.\\n\\nGLOUCESTER:\\nThe widow dainty Menenur, come on; where is your boar-sceneth is to be said,\\nAs thou hast any crazery true dead.\\n\\nJULIET:\\nWhat men devil, fain delight,\\nWherein this law of soon report wranging, you wot well wash \"\n",
            " b\"Romeo slew him, he slew Mercutio;\\nWho now the price of parasion of the senate,\\nAnd with sighs garden lives of all times; he was more than\\nthy enemies. She hath been in all disfigured him by\\nThe Barn till died, and novole playua enemies to give\\nit that he knows how the torghtfry from this spoilt:\\nMy mind presently this time quaffil'd wretch departure\\nWas but the tenthry that my master gold and his eye\\nUppease the dukedom the bottom of the earth, the winds she promised to a cus\\nof some pretty day of women's shier.\\n\\nNORTHUMBERLAND:\\nNow, afore God, 'tis the recorder.\\n\\nFRIAR LAURENCE:\\nSo smile at sack groan's feeling soul and uneventh.\\n\\nGLOUCESTER:\\nHe received in the mid of it?\\n\\nVIRGILIA:\\nI have done. But 'tis the power thus far soul\\nBig at the head of Clarence so ill-addired to his soul!\\nA Pregion of the flatterer; one that I may see my crown is call.\\n\\nBRUTUS:\\nWe stay here to-morrow in your throat:\\nAnd he, that villain seest in your mistress'd sea\\nWhere you shall be my wife; your dowry 'groud?\\n\"\n",
            " b\"Romeo,\\nWho had but newly entertain'd revenge,\\nAnd to make him an execution of my wrongs are but Liby to the bloody king.\\nAnd here he comes.\\nAh, true approaches; and as I love myself.\\n\\nPETRUCHIO:\\nYou peasant swain! you whoreson his word ?\\nWhat would you have me down to do't.\\n\\nFirst Lord:\\nI know my daughter to the vow;\\nSo shall my virtue be his vice's bawd.\\n\\nbather: there's a fear to England's choice; I am one of you,\\nI am thus for a feastion of the sequeces\\nAs the rest of the Capulets lie.\\nIn thereon awake and see in all the veins of censured sinks\\nI promise you, 'tis the worse accuse me. Draw too\\nmighty son:\\nSweet I beseech you, sir, help, here is consent,\\nOr never admidius to woo his grave in quiet, your news,\\nand make you did contemn Berch, smok\\nAnd wailing them to the trial.\\n\\nBUCKINGHAM:\\n-Hent, my prayers for the statue of my sorrow?\\nNow thy proud news with you of your brother, with all the wested sinks\\nAre graced to the purpose did make me fail in our livest;\\nThen pardon him, he has in \"\n",
            " b\"Romeo slew him, he slew Mercutio;\\nWho now the price of cunningbices are Pomperet.\\n\\nBISHOP Edward there?\\n\\nFirst Murderer:\\nI would not be a stander on her;\\nAnd that is false on very slowly.\\n\\nQUEEN ELIZABETH:\\nThat thou hast wronged in the time of death he gave our father.\\n\\nGEORGE:\\nWe stay here forswears to hear you to my brother:\\n'Tis deeds now withal.\\n\\nSOMERSET:\\nAnd do not lord thee not, for one that kill me wrong,\\nAnd therefore here IArly of the dire appointed and true permar,\\nAll this displanted with us else by make me blown to the\\nsoleing; what of that?  rob 'Wis true boats with me to meet his grave\\nAnd him the city gates that kind of England and others must so say\\nit, should-know no longing to the sour?\\n\\nSheraff:\\nMadam, hath been beyond arrived lookers of my prayer\\nTo know the iritest hand of an and breathes;\\nHe thinks that you might leave your best of it.\\n\\nKATHARINA:\\nYour countrymen, my horse.\\n\\nGRUMIO:\\nWhy, so he doth, nor a\\nyour country, and his consent is guess.\\n\\nKING RICHARD III:\\nCate\"\n",
            " b\"Romeo slew him, he slew Mercutio;\\nWho now the price of calonred sleeps,\\nAnd bids them not call upon the ground, and bad seeing\\nmore in a man's blood cleaving to my tongue,\\nAnd watch our voices hereafter so in a mile house!\\n\\nDUKE VINCENTIO:\\nNot a right farewell.\\n\\nSICINIUS:\\nThe fairer love?\\n\\nROMEO:\\nThou charg'd them your worship appear by Edward's hope;\\nAnd charged her then angine and to have her roaring them;\\nAnd so I can scarce and zenomies, but as I am not where.\\n\\nSecond Murderer:\\n'Zounds, a word with you.\\n\\nLUCIO:\\n\\nISABELLA:\\nNot with fond from such a pity say?\\n\\nRICHARD:\\nA mercian, let me twenty good indeed.\\nLook, here Would she kneel with another'd gate,\\nTo take false and treacherous, when I am in holy pawers\\nTo the under hinder of his grave?\\n\\nKING EDWARD IV:\\nIt were no less; but he was banish'd rared\\nMade me so, yet do for which your love to Rome,\\nThey might to the garden lions should be cropp'd be married that we speak,\\nYet he wekes in a fair breath, and heir to be made, but to resign to\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.421740293502808\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4j6dRbJ9997o"
      },
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PdzfIrJO-EjW",
        "outputId": "6e16de79-55bf-46ce-8afd-20db00f323ec"
      },
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Romeo:\n",
            "'Tis thought your heart is full of woman: shall I lead a shows\n",
            "Be rashold as my master slew my brot\n"
          ]
        }
      ]
    }
  ]
}